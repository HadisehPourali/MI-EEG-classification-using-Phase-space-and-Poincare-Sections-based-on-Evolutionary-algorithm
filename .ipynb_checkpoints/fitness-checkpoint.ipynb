{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f10ddb76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200,)\n"
     ]
    }
   ],
   "source": [
    "from geneticalgorithm import geneticalgorithm as ga\n",
    "from ipynb.fs.full.fitness import *\n",
    "from ipynb.fs.full.CSP_filter_data import *\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from ipynb.fs.full.poincareSection import *\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from scipy.stats import variation \n",
    "import warnings\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neighbors import NeighborhoodComponentsAnalysis\n",
    "from sklearn.svm import SVC\n",
    "from ipynb.fs.full.Evaluated import *\n",
    "\n",
    "fvalbest = 1\n",
    "FES = 0\n",
    "chvalbest = []\n",
    "ACCVAL = []\n",
    "ACCTRAIN = []\n",
    "ACCTEST = []\n",
    "\n",
    "def fitness(particle):\n",
    "    \n",
    "    np.seterr(divide='ignore', invalid='ignore')\n",
    "    warnings.simplefilter(\"ignore\", np.ComplexWarning)\n",
    "    \n",
    "    particle = [int(x) for x in particle]\n",
    "    particle[0] = 5\n",
    "    \n",
    "    global FES \n",
    "    print(f\"FES  = {FES}\")\n",
    "\n",
    "    # Calculate the total weight of the dimensions\n",
    "    sumw = 0\n",
    "    weight_start_index = 2\n",
    "    for dim in range(int(particle[0])-1):\n",
    "        sumw = sumw + particle[weight_start_index]\n",
    "        weight_start_index = weight_start_index + 1\n",
    "\n",
    "    f_number = 0\n",
    "    p = 0\n",
    "\n",
    "    #filter data using csp function\n",
    "    fdata, label = CSP_filter_data(xdata,label1)\n",
    "\n",
    "    total_p = 0\n",
    "    for data in range(0, fdata.shape[1] - sumw, particle[1]):\n",
    "        total_p += 1\n",
    "    \n",
    "    number_of_line = 4\n",
    "    number_of_statistical_feature = 17\n",
    "    feature = np.empty((fdata.shape[2], fdata.shape[0] * (2 * number_of_line) * number_of_statistical_feature))\n",
    "    \n",
    "    for segment in range(fdata.shape[2]):  #segment \n",
    "        for channel in range(fdata.shape[0]):  #channel\n",
    "            phase_space = np.empty((total_p, particle[0]))\n",
    "            for data in range(0, fdata.shape[1] - sumw, particle[1]):\n",
    "                phase_space[p, 0] = fdata [channel, data, segment]\n",
    "                next_data = 0\n",
    "                weight_start_index = 2\n",
    "                \n",
    "                for dim in range(1,particle[0]):\n",
    "                    next_data = next_data + particle[weight_start_index]\n",
    "                    phase_space[p,dim] = fdata[channel, data + next_data, segment] \n",
    "                    weight_start_index = weight_start_index + 1 \n",
    "                    \n",
    "                p = p + 1\n",
    "                \n",
    "            n_components=2\n",
    "            pca = PCA(n_components)\n",
    "            phase_space_pca = pca.fit_transform(phase_space)\n",
    "            \n",
    "            minps = np.ones(len(phase_space_pca[0]))\n",
    "            mxps = np.ones(len(phase_space_pca[0]))\n",
    "            for norm in range(len(phase_space_pca[0])):\n",
    "                minps[norm] = min(phase_space_pca[:, norm])\n",
    "                mxps[norm] = max(phase_space_pca[:, norm])\n",
    "\n",
    "            phase_space_pca_norm = np.empty((total_p, len(phase_space_pca[0])))\n",
    "            for norm in range(len(phase_space_pca[0])):\n",
    "                if minps[norm] == mxps[norm]:\n",
    "                    phase_space_pca_norm[:, norm] = phase_space_pca[:, norm] - phase_space_pca[:, norm]\n",
    "                else:\n",
    "                    phase_space_pca_norm[:, norm] = (((phase_space_pca[:, norm] - minps[norm]) / (mxps[norm] - minps[norm])) -0.5 ) * 2 \n",
    "            \n",
    "            a1 = np.ones(number_of_line)\n",
    "            b1 = np.ones(number_of_line)\n",
    "            c1 = np.ones(number_of_line)\n",
    "            \n",
    "            # line y = 0\n",
    "            a1[0] = 0 \n",
    "            b1[0] = 1\n",
    "            c1[0] = 0\n",
    "\n",
    "            # line x = c\n",
    "            a1[1] = 1 \n",
    "            b1[1] = 0\n",
    "            c1[1] = (particle[8] - 1) * (2) / (15) + (-1) # normalize between [-1,1]\n",
    "       \n",
    "            # line y = (a/b) * x + c\n",
    "            a1[2] = (particle[6] - 1) * (2) / (15) + (-1) # normalize between [-1,1]\n",
    "            b1[2] = (particle[7] - 1) * (2) / (15) + (-1) # normalize between [-1,1]\n",
    "            c1[2] = 0\n",
    "        \n",
    "            # line y = -(a/b) * x + c\n",
    "            a1[3] = -(particle[6] - 1) * (2) / (15) + (-1) # normalize between [-1,1]\n",
    "            b1[3] = (particle[7] - 1) * (2) / (15) + (-1) # normalize between [-1,1]\n",
    "            c1[3] = 0\n",
    "\n",
    "            IntersectionPoint = 0 \n",
    "            for line_number in range(len(a1)):\n",
    "                px = np.empty(0)\n",
    "                py = np.empty(0)\n",
    "                for data_number in range(len(phase_space_pca_norm)-1):\n",
    "                    x_axis_value, y_axis_value, pbool = poincareSection(phase_space_pca_norm[data_number, 0], phase_space_pca_norm[data_number, 1], phase_space_pca_norm[data_number + 1, 0], phase_space_pca_norm[data_number + 1, 1], a1[line_number], b1[line_number], c1[line_number])\n",
    "                    \n",
    "                    if pbool == 1:\n",
    "                        #IntersectionPoint = IntersectionPoint + 1\n",
    "                        px = np.append(px, x_axis_value)\n",
    "                        py = np.append(py, y_axis_value)  \n",
    "\n",
    "                if len(px) == 0 :\n",
    "                    px = np.array([0, 0])\n",
    "                    py = np.array([0, 0.1])\n",
    "                \n",
    "                px = [float(x) for x in px]\n",
    "                feature[segment, f_number] = max(px)- min(px) #range\n",
    "                feature[segment, f_number + 1] = np.quantile(px, 0.13) #quantile\n",
    "                feature[segment, f_number + 2] = np.quantile(px, 0.25) - np.quantile(px, 0.75) #interquartile\n",
    "                px_series = pd.Series(px)\n",
    "                counts = px_series.value_counts()\n",
    "                feature[segment, f_number + 3] = scipy.stats.entropy(counts)  #shannon entropy\n",
    "                feature[segment, f_number + 4] = np.sqrt(np.mean(np.power(px, 2))) #root mean squared\n",
    "                feature[segment, f_number + 5] = variation(px) #Coefficient of Variation\n",
    "                feature[segment, f_number + 6] = pow(sum(px),2) #energy\n",
    "                feature[segment, f_number + 7] = np.var(px) #variance\n",
    "            \n",
    "                feature[segment, f_number + 8] = max(py)- min(py) #range\n",
    "                feature[segment, f_number + 9] = np.quantile(py, 0.13) #quantile\n",
    "                feature[segment, f_number + 10] = np.quantile(py, 0.25) - np.quantile(py, 0.75) #interquartile\n",
    "                py_series = pd.Series(py)\n",
    "                counts = py_series.value_counts()\n",
    "                feature[segment, f_number + 11] = scipy.stats.entropy(counts)  #shannon entropy\n",
    "                feature[segment, f_number + 12] = np.sqrt(np.mean(np.power(py, 2))) #root mean squared\n",
    "                feature[segment, f_number + 13] = variation(py) #Coefficient of Variation\n",
    "                feature[segment, f_number + 14] = pow(sum(py),2) #energy\n",
    "                feature[segment, f_number + 15] = np.var(py) #variance\n",
    "                feature[segment, f_number + 16] = len(px)\n",
    "\n",
    "                f_number = f_number + 17\n",
    "                del px, py\n",
    "                IntersectionPoint = 1\n",
    "\n",
    "            del phase_space, phase_space_pca, pca, phase_space_pca_norm\n",
    "            p = 0\n",
    "\n",
    "        f_number = 0\n",
    "\n",
    "    # normalization [-1,+1]\n",
    "    minfr = np.ones(len(feature[0]))\n",
    "    mxfr = np.ones(len(feature[0]))\n",
    "    for norm in range(len(feature[0])):\n",
    "        minfr[norm] = min(feature[:, norm])\n",
    "        mxfr[norm] = max(feature[:, norm])\n",
    "\n",
    "    featurenorm = np.empty((len(feature), len(feature[0])))\n",
    "    for norm in range(len(feature[0])):\n",
    "        if (round(minfr[norm],7)== round(mxfr[norm],7)):\n",
    "            featurenorm[:, norm] = feature[:, norm] - feature[:, norm]\n",
    "        else:\n",
    "            featurenorm[:, norm] =(((feature[:, norm] - minfr[norm]) / (mxfr[norm] - minfr[norm])) - 0.5) * 2 \n",
    "\n",
    "    # delete fix and nan column\n",
    "    [row, col] = np.where(np.isnan(featurenorm))\n",
    "    featureout = np.empty((len(featurenorm),0))\n",
    "    for i in range(len(featurenorm[0])):\n",
    "        min1 = min(featurenorm[:, i])\n",
    "        max1 = max(featurenorm[:, i])\n",
    "        if (min1 < max1 and np.isin(i,col) == 0):\n",
    "            temp = featurenorm[:, i]\n",
    "            featureout = np.append(featureout,temp[:,None],1)\n",
    "\n",
    "    X = featureout\n",
    "    Y = label\n",
    "    Y = Y.T\n",
    "\n",
    "    yptrain = []\n",
    "    ytrain = []\n",
    "\n",
    "    yptest = []\n",
    "    ytest = []\n",
    "\n",
    "    ypval = []\n",
    "    yval = []\n",
    "\n",
    "    #nested cross validation\n",
    "    cv_outer = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "    for trainvalindex, testIdx in cv_outer.split(X):\n",
    "        Xtrainvalidation = X[trainvalindex,:]\n",
    "        Ytrainvalidation = Y[trainvalindex]\n",
    "\n",
    "        Xtest = X[testIdx,:]\n",
    "        Ytest = Y[testIdx]\n",
    "        Ytest = Ytest.T\n",
    "\n",
    "        mdl = NeighborhoodComponentsAnalysis(tol = 0.02)\n",
    "        mdl.fit(Xtrainvalidation, Ytrainvalidation)\n",
    "        \n",
    "        cv_inner = KFold(n_splits=4, shuffle=True, random_state=1)\n",
    "        bestvalidationacc = 0\n",
    "    \n",
    "        for trainindex, valIdx in cv_inner.split(Xtrainvalidation):\n",
    "            \n",
    "            Xtrain = Xtrainvalidation[trainindex,:]\n",
    "            Ytrain = Ytrainvalidation[trainindex]\n",
    "            Ytrain = Ytrain.T\n",
    "            Xval = Xtrainvalidation[valIdx,:]\n",
    "            Yval = Ytrainvalidation[valIdx]\n",
    "            Yval = Yval.T\n",
    "\n",
    "            mdl1 = NeighborhoodComponentsAnalysis(tol = 0.02)\n",
    "            mdl1.fit(Xtrain, Ytrain)\n",
    "            \n",
    "            model = SVC(kernel = 'linear')\n",
    "            model.fit(mdl1.transform(Xtrain), Ytrain)\n",
    "            \n",
    "            ytrainp = model.predict(mdl1.transform(Xtrain))\n",
    "            \n",
    "            yptrain.append(list(ytrainp))\n",
    "            ytrain.append(list(Ytrain))\n",
    "\n",
    "            yvalp = model.predict(mdl1.transform(Xval))\n",
    "            \n",
    "            ypval.append(list(yvalp))\n",
    "            yval.append(list(Yval))\n",
    "        \n",
    "            del model, mdl1\n",
    "\n",
    "        modelout = SVC(kernel = 'linear')\n",
    "        modelout.fit(mdl.transform(Xtrainvalidation), Ytrainvalidation)\n",
    "        ytestp = modelout.predict(mdl.transform(Xtest))\n",
    "        \n",
    "        yptest.append(list(ytestp))\n",
    "        ytest.append(list(Ytest))\n",
    "        \n",
    "        del modelout, mdl\n",
    "          \n",
    "    accvaltest = Evaluated(ytest,yptest,1)\n",
    "    accvaltrain = Evaluated(ytrain,yptrain,1)\n",
    "    accvalvalidation = Evaluated(yval,ypval,1)\n",
    "\n",
    "    out = 1 - accvalvalidation # validation error\n",
    "    \n",
    "    global ACCVAL, ACCTRAIN, ACCTEST, fvalbest\n",
    "\n",
    "    if(out < fvalbest):\n",
    "        fvalbest = out\n",
    "        chvalbest = particle\n",
    "        ACCTRAIN.append(accvaltrain) \n",
    "        ACCTEST.append(accvaltest)\n",
    "        ACCVAL.append(accvalvalidation) \n",
    "        print(f\"The best validation accuracy ever achieved is {accvalvalidation}\")\n",
    "    else:\n",
    "        ACCTRAIN.append(ACCTRAIN[FES - 1]) \n",
    "        ACCTEST.append(ACCTEST[FES - 1])\n",
    "        ACCVAL.append(ACCVAL[FES - 1]) \n",
    "        \n",
    "    FES = FES + 1\n",
    "    \n",
    "    return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "baac65c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
